{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, logging as hf_logging\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "TRAIN_CONFIG = {'MODEL_NAME': 'sentence-transformers/all-mpnet-base-v2', 'MAX_LENGTH': 512, 'BATCH_SIZE': 32, 'EPOCHS': 10, 'LR_ENCODER': 2e-06, 'LR_HEAD': 2e-05, 'N_SPLITS': 5, 'SEED': 42, \n",
    "                'DEVICE': torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "                }\n",
    "METRIC_EMBEDDING_DIMENSION = 768\n",
    "INTERACTION_FEATURE_DIMENSION = METRIC_EMBEDDING_DIMENSION * 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868c871",
   "metadata": {},
   "source": [
    "Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed_value: int) -> None:\n",
    "\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def compute_expected_score_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    class_probabilities = torch.softmax(logits, dim=1)\n",
    "    score_values = torch.arange(0, 11, dtype=torch.float, device=class_probabilities.device)\n",
    "    expected_scores = (class_probabilities * score_values).sum(dim=1)\n",
    "    return expected_scores\n",
    "\n",
    "set_random_seed(TRAIN_CONFIG['SEED'])\n",
    "print(f\"Using device: {TRAIN_CONFIG['DEVICE']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649afe0",
   "metadata": {},
   "source": [
    "Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe = pd.read_json('train_data.json')\n",
    "metric_name_list = pd.read_json('metric_names.json')\n",
    "metric_name_embedding_matrix = np.load('metric_name_embeddings.npy')\n",
    "metric_name_to_embedding = dict(zip(metric_name_list, metric_name_embedding_matrix))\n",
    "training_dataframe['user_prompt'] = training_dataframe['user_prompt'].astype(str)\n",
    "training_dataframe['system_prompt'] = training_dataframe['system_prompt'].fillna('None').astype(str)\n",
    "training_dataframe['response'] = training_dataframe['response'].astype(str)\n",
    "training_dataframe['full_text'] = 'User: ' + training_dataframe['user_prompt'] + ' | System: ' + training_dataframe['system_prompt'] + ' | Response: ' + training_dataframe['response']\n",
    "training_dataframe['metric_embedding'] = training_dataframe['metric_name'].map(metric_name_to_embedding)\n",
    "training_dataframe['score_float'] = training_dataframe['score'].astype(float)\n",
    "training_dataframe['score_label'] = training_dataframe['score_float'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a982db2",
   "metadata": {},
   "source": [
    "Class Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ad587",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_label_counts = training_dataframe['score_label'].value_counts().sort_index().reindex(range(11), fill_value=0)\n",
    "num_training_samples = len(training_dataframe)\n",
    "num_score_classes = 11\n",
    "raw_class_weight_values = num_training_samples / (num_score_classes * (score_label_counts + 1))\n",
    "class_weight_tensor = torch.tensor(raw_class_weight_values.values, dtype=torch.float).to(TRAIN_CONFIG['DEVICE'])\n",
    "class_weight_array = class_weight_tensor.cpu().numpy()\n",
    "\n",
    "for score_value in range(6):\n",
    "    class_weight_array[score_value] = class_weight_array[score_value] * 1.5\n",
    "    \n",
    "class_weight_tensor = torch.tensor(class_weight_array, dtype=torch.float).to(TRAIN_CONFIG['DEVICE']) \n",
    "print('Using class weights for imbalance compensation.')\n",
    "print('NOTE: Manually boosted class weights for scores 0-5 by 1.5x.') \n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TRAIN_CONFIG['MODEL_NAME'])  # Initializing tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca96511",
   "metadata": {},
   "source": [
    "Pytorch Dataset and Model Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f028041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetricScoringDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_frame: pd.DataFrame, tokenizer, max_length: int) -> None:\n",
    "        \n",
    "        self.sample_texts = data_frame['full_text'].values\n",
    "        metric_embedding_series = data_frame['metric_embedding'].apply(lambda embedding: embedding if isinstance(embedding, np.ndarray) else np.zeros(METRIC_EMBEDDING_DIMENSION))\n",
    "        self.metric_embeddings = np.stack(metric_embedding_series.values)\n",
    "        self.score_labels = data_frame['score_label'].values\n",
    "        self.score_values = data_frame['score_float'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return len(self.sample_texts)\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "\n",
    "        text_sample = self.sample_texts[index]\n",
    "        tokenized_inputs = self.tokenizer.encode_plus(text_sample, None, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        input_ids = tokenized_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = tokenized_inputs['attention_mask'].squeeze(0)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'metric_embedding': torch.tensor(self.metric_embeddings[index], dtype=torch.float), 'label': torch.tensor(self.score_labels[index], dtype=torch.long), 'score': torch.tensor(self.score_values[index], dtype=torch.float)}\n",
    "\n",
    "class InferenceMetricScoringDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_frame: pd.DataFrame, tokenizer, max_length: int) -> None:\n",
    "\n",
    "        self.sample_texts = data_frame['full_text'].values\n",
    "        metric_embedding_series = data_frame['metric_embedding'].apply(lambda embedding: embedding if isinstance(embedding, np.ndarray) else np.zeros(METRIC_EMBEDDING_DIMENSION))\n",
    "        self.metric_embeddings = np.stack(metric_embedding_series.values)\n",
    "        self.sample_ids = data_frame['ID'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        return len(self.sample_texts)\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "\n",
    "        text_sample = self.sample_texts[index]\n",
    "        tokenized_inputs = self.tokenizer.encode_plus(text_sample, None, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        input_ids = tokenized_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = tokenized_inputs['attention_mask'].squeeze(0)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'metric_embedding': torch.tensor(self.metric_embeddings[index], dtype=torch.float), 'id': torch.tensor(self.sample_ids[index], dtype=torch.long)}\n",
    "\n",
    "class MetricScoringModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name: str) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.text_encoder = SentenceTransformer(model_name)\n",
    "        self.prediction_head = nn.Sequential(nn.Linear(INTERACTION_FEATURE_DIMENSION, 1024), nn.BatchNorm1d(1024), nn.ReLU(), nn.Dropout(0.4), nn.Linear(1024, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, 11))\n",
    "\n",
    "    def forward(self, metric_embedding_batch: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        batch_text_inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        text_embedding_output = self.text_encoder(batch_text_inputs)\n",
    "        text_embedding_batch = text_embedding_output['sentence_embedding']\n",
    "        metric_embedding_batch = metric_embedding_batch\n",
    "        embedding_difference = torch.abs(metric_embedding_batch - text_embedding_batch)\n",
    "        embedding_product = metric_embedding_batch * text_embedding_batch\n",
    "        interaction_features = torch.cat([metric_embedding_batch, text_embedding_batch, embedding_difference, embedding_product], dim=1)\n",
    "        logits = self.prediction_head(interaction_features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bbeb7",
   "metadata": {},
   "source": [
    "Training and Evaluation Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bedbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model: nn.Module, data_loader: DataLoader, optim_engine: optim.Optimizer, loss_fn, device: torch.device) -> float:\n",
    "\n",
    "    model.train()\n",
    "    cumulative_loss = 0.0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        optim_engine.zero_grad()\n",
    "        batch_input_ids = batch['input_ids'].to(device)\n",
    "        batch_attention_mask = batch['attention_mask'].to(device)\n",
    "        batch_metric_embeddings = batch['metric_embedding'].to(device)\n",
    "        batch_labels = batch['label'].to(device)\n",
    "        logits = model(batch_metric_embeddings, batch_input_ids, batch_attention_mask)\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optim_engine.step()\n",
    "        cumulative_loss += loss.item()\n",
    "    mean_epoch_loss = cumulative_loss / len(data_loader)\n",
    "\n",
    "    return mean_epoch_loss\n",
    "\n",
    "def eval_fn(model: nn.Module, data_loader: DataLoader, loss_fn, device: torch.device) -> tuple[float, float]:\n",
    "\n",
    "    model.eval()\n",
    "    cumulative_loss = 0.0\n",
    "    predicted_scores_list = []\n",
    "    true_scores_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "\n",
    "            batch_input_ids = batch['input_ids'].to(device)\n",
    "            batch_attention_mask = batch['attention_mask'].to(device)\n",
    "            batch_metric_embeddings = batch['metric_embedding'].to(device)\n",
    "            batch_labels = batch['label'].to(device)\n",
    "            batch_scores = batch['score'].to(device)\n",
    "            logits = model(batch_metric_embeddings, batch_input_ids, batch_attention_mask)\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            cumulative_loss += loss.item()\n",
    "            \n",
    "            expected_scores = compute_expected_score_from_logits(logits)\n",
    "            predicted_scores_list.extend(expected_scores.cpu().numpy())\n",
    "            true_scores_list.extend(batch_scores.cpu().numpy())\n",
    "\n",
    "    validation_rmse = np.sqrt(mean_squared_error(true_scores_list, predicted_scores_list))\n",
    "    mean_validation_loss = cumulative_loss / len(data_loader)\n",
    "\n",
    "    return (mean_validation_loss, validation_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe0e8f",
   "metadata": {},
   "source": [
    "K-Fold Cross-Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- Starting Model Training ---')\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weight_tensor)\n",
    "cross_validation_splitter = StratifiedGroupKFold(n_splits=TRAIN_CONFIG['N_SPLITS'], shuffle=True, random_state=TRAIN_CONFIG['SEED'])\n",
    "\n",
    "sample_indices = training_dataframe.index\n",
    "score_label_series = training_dataframe['score_label']\n",
    "metric_group_labels = training_dataframe['metric_name']\n",
    "fold_rmse_scores = []\n",
    "fold_model_paths = [f'model_fold_{fold_index + 1}.pth' for fold_index in range(4)]\n",
    "\n",
    "for fold_index, (train_indices, validation_indices) in enumerate(cross_validation_splitter.split(sample_indices, score_label_series, metric_group_labels)):\n",
    "    if fold_index < 5:\n",
    "        print(f'\\n--- Skipping Fold {fold_index + 1} (Already Trained) ---')\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Fold {fold_index + 1}/{TRAIN_CONFIG['N_SPLITS']} ---\")\n",
    "\n",
    "    train_fold_dataframe = training_dataframe.iloc[train_indices]\n",
    "    validation_fold_dataframe = training_dataframe.iloc[validation_indices]\n",
    "    train_dataset = TrainingMetricScoringDataset(train_fold_dataframe, text_tokenizer, TRAIN_CONFIG['MAX_LENGTH'])\n",
    "    validation_dataset = TrainingMetricScoringDataset(validation_fold_dataframe, text_tokenizer, TRAIN_CONFIG['MAX_LENGTH'])\n",
    "    training_loader = DataLoader(train_dataset, batch_size=TRAIN_CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2, drop_last=True)\n",
    "    validation_data_loader = DataLoader(validation_dataset, batch_size=TRAIN_CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f'Train samples: {len(train_fold_dataframe)}, Val samples: {len(validation_fold_dataframe)}')\n",
    "    \n",
    "    metric_model = MetricScoringModel(TRAIN_CONFIG['MODEL_NAME']).to(TRAIN_CONFIG['DEVICE'])\n",
    "    optim_engine = optim.AdamW([{'params': metric_model.text_encoder.parameters(), 'lr': TRAIN_CONFIG['LR_ENCODER']}, {'params': metric_model.prediction_head.parameters(), 'lr': TRAIN_CONFIG['LR_HEAD']}])\n",
    "    best_fold_rmse = float('inf')\n",
    "    fold_model_path = f'model_fold_{fold_index + 1}.pth'\n",
    "\n",
    "    for epoch_idx in range(TRAIN_CONFIG['EPOCHS']):\n",
    "\n",
    "        training_loss = train_fn(metric_model, training_loader, optim_engine, loss_fn, TRAIN_CONFIG['DEVICE'])\n",
    "        validation_loss, validation_rmse = eval_fn(metric_model, validation_data_loader, loss_fn, TRAIN_CONFIG['DEVICE'])\n",
    "        print(f'Epoch {epoch_idx + 1}: Train Loss={training_loss:.4f} | Val Loss={validation_loss:.4f} | Val RMSE={validation_rmse:.4f}')\n",
    "        if validation_rmse < best_fold_rmse:\n",
    "            best_fold_rmse = validation_rmse\n",
    "            torch.save(metric_model.state_dict(), fold_model_path)\n",
    "            print(f'  ^ New best model saved to {fold_model_path}')\n",
    "\n",
    "    fold_rmse_scores.append(best_fold_rmse)\n",
    "    fold_model_paths.append(fold_model_path)\n",
    "    print(f'Best RMSE for Fold {fold_index + 1}: {best_fold_rmse:.4f}')\n",
    "    del metric_model, training_loader, validation_data_loader, train_dataset, validation_dataset\n",
    "    gc.collect()\n",
    "    \n",
    "    if TRAIN_CONFIG['DEVICE'] == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print('\\n--- Training Complete ---')\n",
    "print(f'RMSE for new folds: {fold_rmse_scores}')\n",
    "print(f'All model paths for ensemble: {fold_model_paths}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ec0c4",
   "metadata": {},
   "source": [
    "Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- Starting Inference on Test Data ---')\n",
    "fold_model_paths = [f'model_fold_{fold_index + 1}.pth' for fold_index in range(10)]\n",
    "\n",
    "INFERENCE_CONFIG = {'MODEL_NAME': 'sentence-transformers/all-mpnet-base-v2', 'MAX_LENGTH': 512, 'BATCH_SIZE': 32, 'DEVICE': torch.device('cpu')}\n",
    "test_dataframe = pd.read_json('test_data.json')\n",
    "\n",
    "if 'ID' not in test_dataframe.columns:\n",
    "    test_dataframe['ID'] = range(1, len(test_dataframe) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07f851",
   "metadata": {},
   "source": [
    "Test Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing test data...')\n",
    "test_dataframe['user_prompt'] = test_dataframe['user_prompt'].astype(str)\n",
    "test_dataframe['system_prompt'] = test_dataframe['system_prompt'].fillna('None').astype(str)\n",
    "test_dataframe['response'] = test_dataframe['response'].astype(str)\n",
    "test_dataframe['full_text'] = 'User: ' + test_dataframe['user_prompt'] + ' | System: ' + test_dataframe['system_prompt'] + ' | Response: ' + test_dataframe['response']\n",
    "test_dataframe['metric_embedding'] = test_dataframe['metric_name'].map(metric_name_to_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dabbb4",
   "metadata": {},
   "source": [
    "Dataset, DataLoader, and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = InferenceMetricScoringDataset(test_dataframe, text_tokenizer, INFERENCE_CONFIG['MAX_LENGTH'])\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=INFERENCE_CONFIG['BATCH_SIZE'] * 2, shuffle=False, num_workers=2)\n",
    "all_fold_probability_arrays = []\n",
    "inference_model = MetricScoringModel(INFERENCE_CONFIG['MODEL_NAME']).to(INFERENCE_CONFIG['DEVICE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed20fd",
   "metadata": {},
   "source": [
    "Ensemble Inference and Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfdcdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_model_path in fold_model_paths:\n",
    "\n",
    "    print(f'Loading model: {fold_model_path}')\n",
    "    try:\n",
    "        inference_model.load_state_dict(torch.load(fold_model_path, map_location=INFERENCE_CONFIG['DEVICE']), strict=False)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Warning: Model file {fold_model_path} not found. Skipping this fold.')\n",
    "        continue\n",
    "    inference_model.eval()\n",
    "    fold_probability_batches = []\n",
    "\n",
    "    for batch in tqdm(test_data_loader, desc=f'Predicting with {fold_model_path}'):\n",
    "        \n",
    "        batch_input_ids = batch['input_ids'].to(INFERENCE_CONFIG['DEVICE'])\n",
    "        batch_attention_mask = batch['attention_mask'].to(INFERENCE_CONFIG['DEVICE'])\n",
    "        batch_metric_embeddings = batch['metric_embedding'].to(INFERENCE_CONFIG['DEVICE'])\n",
    "        logits = inference_model(batch_metric_embeddings, batch_input_ids, batch_attention_mask)\n",
    "        batch_probabilities = torch.softmax(logits, dim=1)\n",
    "        fold_probability_batches.append(batch_probabilities.detach().cpu().numpy())\n",
    "    all_fold_probability_arrays.append(np.concatenate(fold_probability_batches))\n",
    "\n",
    "if not all_fold_probability_arrays:\n",
    "    print('Error: No models were loaded for inference. Cannot create the file.')\n",
    "    \n",
    "else:\n",
    "    print('Ensemble predictions complete.')\n",
    "    averaged_class_probabilities = np.mean(all_fold_probability_arrays, axis=0)\n",
    "    score_value_array = np.arange(0, 11)\n",
    "    predicted_score_values = (averaged_class_probabilities * score_value_array).sum(axis=1)\n",
    "    submission_dataframe = pd.DataFrame({'ID': test_dataframe['ID'], 'score': predicted_score_values})\n",
    "    submission_dataframe['score'] = submission_dataframe['score'].clip(0, 10)\n",
    "    submission_dataframe.to_csv('FINAL_RUN.csv', index=False)\n",
    "    \n",
    "    print('\\n--- FINAL_RUN.csv created successfully! ---')\n",
    "    print(submission_dataframe.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVS25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
